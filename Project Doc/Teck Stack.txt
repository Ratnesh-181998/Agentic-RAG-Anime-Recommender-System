Tech stack :
1.	Groq --> LLM
2.	HuggingFace --> Embedding Model
3.	Langchain --> Generative AI Framework to interact with LLM
4.	GCP VM --> Virtual Machine that can be accessed on cloud. It's a service offered by Google Cloud.
5.	Minikube --> For making a Kubernetes Cluster where you can deploy your application
6.	Streamlit --> To make UI or frontend of the app
7.	Docker --> For containerization of the app during deployment
8.	Grafana Cloud --> Monitoring your Kubernetes Clusters
9.	Chroma DB --> Local Vector Store for storing Embeddings
10.	Kubectl --> Command Line Interface to interact with your Kubernetes
11.	GitHub --> It will work as a SCM for your project.


AI Anime Recommender system.
________________________________________
ğŸ”§ Tech Stack â€“ Step-by-Step Explanation (End-to-End)
Think of this project in 5 layers:
1.	AI / Intelligence
2.	Data & Retrieval
3.	Application Layer
4.	Deployment & Infrastructure
5.	Monitoring & DevOps
________________________________________
ğŸ§  1ï¸âƒ£ Groq â†’ LLM (Brain of the System)
Role:
Groq provides the Large Language Model (LLM) used for:
â€¢	Understanding user queries
â€¢	Reasoning over retrieved anime data
â€¢	Generating human-like recommendations
Why Groq?
â€¢	Ultra-low latency inference
â€¢	Ideal for real-time recommendations
â€¢	Much faster than traditional GPU inference
Example:
User says:
â€œSuggest anime like Naruto but darker and matureâ€
â¡ Groq LLM understands intent, tone, and constraints
________________________________________
ğŸ§© 2ï¸âƒ£ Hugging Face â†’ Embedding Model (Semantic Understanding)
Role:
Converts text into numerical vectors (embeddings).
Used for:
â€¢	Anime synopsis embeddings
â€¢	Genre & theme embeddings
â€¢	User preference embeddings
Why embeddings?
â€¢	Enables semantic search
â€¢	Finds meaning, not exact keywords
Example:
"Ninja, dark fantasy, action"
â†’ [0.023, 0.981, -0.442, ...]
________________________________________
ğŸ”— 3ï¸âƒ£ LangChain â†’ Generative AI Framework
Role:
LangChain acts as the orchestrator between:
â€¢	LLM (Groq)
â€¢	Embeddings (Hugging Face)
â€¢	Vector DB (Chroma)
What it does:
â€¢	Builds chains (prompt â†’ retrieval â†’ reasoning â†’ response)
â€¢	Handles context injection
â€¢	Enables RAG (Retrieval Augmented Generation)
Flow:
User Query
 â†’ Embedding
 â†’ Vector Search
 â†’ Context Injection
 â†’ LLM Response
________________________________________
ğŸ§  4ï¸âƒ£ Chroma DB â†’ Vector Store (Memory Layer)
Role:
Stores anime embeddings and performs similarity search.
Why Chroma?
â€¢	Lightweight
â€¢	Fast local vector search
â€¢	Perfect for prototyping & demos
Stored Data:
â€¢	Anime title
â€¢	Synopsis
â€¢	Genres
â€¢	Embedding vector
Query Example:
Find top-10 anime vectors similar to user preference vector
________________________________________
ğŸ–¥ï¸ 5ï¸âƒ£ Streamlit â†’ Frontend / UI
Role:
Creates the user interface for the recommender.
What user sees:
â€¢	Input box (favorite anime / mood)
â€¢	Recommendation list
â€¢	Natural language explanation
Why Streamlit?
â€¢	Fast to build
â€¢	Python-native
â€¢	Ideal for ML/AI demos
________________________________________
âš™ï¸ 6ï¸âƒ£ Docker â†’ Containerization
Role:
Packages the entire app into containers.
Why Docker?
â€¢	Same environment everywhere
â€¢	Easy deployment
â€¢	Required for Kubernetes
Container Includes:
â€¢	Streamlit app
â€¢	LangChain logic
â€¢	Chroma DB
â€¢	Model clients
________________________________________
â˜¸ï¸ 7ï¸âƒ£ Minikube â†’ Local Kubernetes Cluster
Role:
Runs Kubernetes locally for testing.
Why Minikube?
â€¢	Simulates production-grade Kubernetes
â€¢	Ideal for development & learning
What runs inside Minikube:
â€¢	App pods
â€¢	Service networking
â€¢	Autoscaling configs
________________________________________
â˜ï¸ 8ï¸âƒ£ GCP VM â†’ Cloud Infrastructure
Role:
Provides a Virtual Machine on Google Cloud where:
â€¢	Docker
â€¢	Minikube / Kubernetes
â€¢	Application runs
Why GCP VM?
â€¢	Cloud accessibility
â€¢	Scalable
â€¢	Production-like environment
________________________________________
ğŸ“Š 9ï¸âƒ£ Grafana Cloud â†’ Monitoring
Role:
Monitors:
â€¢	API latency
â€¢	Pod health
â€¢	Resource usage
â€¢	Error rates
Why Grafana?
â€¢	Visual dashboards
â€¢	Production-ready observability
â€¢	Kubernetes-friendly
________________________________________
âŒ¨ï¸ 1ï¸âƒ£0ï¸âƒ£ Kubectl â†’ Kubernetes CLI
Role:
Command-line tool to manage Kubernetes.
Used for:
â€¢	Deploying pods
â€¢	Checking logs
â€¢	Scaling services
Example:
kubectl get pods
kubectl logs anime-recommender-pod
________________________________________
ğŸŒ 1ï¸âƒ£1ï¸âƒ£ GitHub â†’ Source Control (SCM)
Role:
Manages:
â€¢	Code versions
â€¢	Collaboration
â€¢	CI/CD readiness
Why GitHub?
â€¢	Industry standard
â€¢	Easy integration with DevOps tools
________________________________________
ğŸ” Complete Execution Flow (One View)
User (Streamlit UI)
 â†’ LangChain
   â†’ HuggingFace Embeddings
     â†’ Chroma DB (Vector Search)
   â†’ Groq LLM (Reasoning)
 â†’ Final Recommendation
 â†’ Monitored by Grafana
 â†’ Deployed via Docker + Kubernetes (Minikube on GCP VM)
________________________________________
âœ…  One-Line Summary
â€œThis system uses Groq LLM with LangChain-based RAG architecture, Hugging Face embeddings stored in Chroma DB for semantic anime recommendations, deployed using Docker and Kubernetes on GCP, with real-time monitoring via Grafana Cloud.â€
